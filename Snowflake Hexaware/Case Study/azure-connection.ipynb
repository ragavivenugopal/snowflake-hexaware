{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b8dee4e-38e2-48a0-8ce5-4114d08dd81b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snowflake-snowpark-python\n",
      "  Downloading snowflake_snowpark_python-1.40.0-py3-none-any.whl.metadata (170 kB)\n",
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: setuptools>=40.6.0 in /usr/local/lib/python3.12/dist-packages (from snowflake-snowpark-python) (74.0.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (from snowflake-snowpark-python) (0.45.1)\n",
      "Collecting snowflake-connector-python<4.0.0,>=3.17.0 (from snowflake-snowpark-python)\n",
      "  Downloading snowflake_connector_python-3.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (74 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /databricks/python3/lib/python3.12/site-packages (from snowflake-snowpark-python) (4.12.2)\n",
      "Requirement already satisfied: pyyaml in /databricks/python3/lib/python3.12/site-packages (from snowflake-snowpark-python) (6.0.2)\n",
      "Requirement already satisfied: cloudpickle!=2.1.0,!=2.2.0,<=3.1.1,>=1.6.0 in /databricks/python3/lib/python3.12/site-packages (from snowflake-snowpark-python) (3.0.0)\n",
      "Requirement already satisfied: protobuf<6.32,>=3.20 in /databricks/python3/lib/python3.12/site-packages (from snowflake-snowpark-python) (5.29.4)\n",
      "Requirement already satisfied: python-dateutil in /databricks/python3/lib/python3.12/site-packages (from snowflake-snowpark-python) (2.9.0.post0)\n",
      "Collecting tzlocal (from snowflake-snowpark-python)\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil->snowflake-snowpark-python) (1.16.0)\n",
      "Collecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python)\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: boto3>=1.24 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (1.36.2)\n",
      "Requirement already satisfied: botocore>=1.24 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (1.36.3)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (1.17.1)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (43.0.3)\n",
      "Collecting pyOpenSSL<26.0.0,>=22.0.0 (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python)\n",
      "  Downloading pyopenssl-25.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (2.10.1)\n",
      "Requirement already satisfied: requests<3.0.0 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (2.32.3)\n",
      "Requirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (24.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (2025.1.31)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (3.18.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (3.10.0)\n",
      "Collecting tomlkit (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /databricks/python3/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (0.11.3)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /databricks/python3/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (2.3.0)\n",
      "Requirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python) (2.21)\n",
      "Collecting cryptography>=3.1.0 (from snowflake-connector-python<4.0.0,>=3.17.0->snowflake-snowpark-python)\n",
      "  Downloading cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "INFO: pip is looking at multiple versions of cryptography to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading cryptography-46.0.2-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "  Downloading cryptography-46.0.1-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "  Downloading cryptography-46.0.0-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Downloading snowflake_snowpark_python-1.40.0-py3-none-any.whl (1.8 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading snowflake_connector_python-3.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Downloading pyopenssl-25.3.0-py3-none-any.whl (57 kB)\n",
      "Downloading cryptography-46.0.0-cp311-abi3-manylinux_2_34_x86_64.whl (4.6 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: asn1crypto, tzlocal, tomlkit, cryptography, pyOpenSSL, snowflake-connector-python, snowflake-snowpark-python\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 43.0.3\n",
      "    Not uninstalling cryptography at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e4cc98c4-af43-447b-bda5-d35da5d52c9d\n",
      "    Can't uninstall 'cryptography'. No files were found to uninstall.\n",
      "Successfully installed asn1crypto-1.5.1 cryptography-46.0.0 pyOpenSSL-25.3.0 snowflake-connector-python-3.18.0 snowflake-snowpark-python-1.40.0 tomlkit-0.13.3 tzlocal-5.3.1\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (UPDATED VERSIONS)\n",
    "%pip install snowflake-snowpark-python pandas\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed903727-c932-484d-8c32-9716c2c4376e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and configuration\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.types import *\n",
    "\n",
    "# Snowflake connection details\n",
    "snowflake_account = \"TYMHDZV-PZ92491\"\n",
    "snowflake_user = \"Ruthra\" \n",
    "snowflake_password = \"Your pass\"\n",
    "\n",
    "# Azure storage details\n",
    "storage_account = \"ittechgeniestorage\"\n",
    "container_name = \"sales-data\"\n",
    "\n",
    "# Snowflake objects\n",
    "warehouse = \"COMPUTE_WH\"\n",
    "database = \"ITTG_SALES_DB\"\n",
    "raw_schema = \"RAW_DATA\"\n",
    "clean_schema = \"CLEAN_DATA\"\n",
    "\n",
    "connection_parameters = {\n",
    "    \"account\": \"TYMHDZV-PZ92491\",\n",
    "    \"user\": \"Ruthra\",\n",
    "    \"password\": \"Ruthra#Your pass\",\n",
    "    \"role\": \"ACCOUNTADMIN\",\n",
    "    \"warehouse\": \"COMPUTE_WH\",\n",
    "    \"database\": \"ITTG_SALES_DB\",\n",
    "    \"schema\": \"RAW_DATA\"\n",
    "}\n",
    "\n",
    "print(\"Configuration set successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84cfb041-ddfb-480e-9481-e27336535882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowpark session created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create Snowpark session\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(\"Snowpark session created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38352990-5531-4188-860f-f0f1908af518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database and schema setup completed\n"
     ]
    }
   ],
   "source": [
    "# Create database and schema if not exists\n",
    "session.sql(\"CREATE DATABASE IF NOT EXISTS ITTG_SALES_DB\").collect()\n",
    "session.sql(\"CREATE SCHEMA IF NOT EXISTS ITTG_SALES_DB.RAW_DATA\").collect()\n",
    "session.sql(\"CREATE SCHEMA IF NOT EXISTS ITTG_SALES_DB.CLEAN_DATA\").collect()\n",
    "session.sql(\"USE DATABASE ITTG_SALES_DB\").collect()\n",
    "session.sql(\"USE SCHEMA RAW_DATA\").collect()\n",
    "print(\"Database and schema setup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9613774-2319-4919-80b4-aa1d308d0fd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File format and stage created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create file format and stage WITH SAS TOKEN\n",
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE FILE FORMAT csv_sales_format\n",
    "    TYPE = 'CSV'\n",
    "    FIELD_DELIMITER = ','\n",
    "    SKIP_HEADER = 1\n",
    "    NULL_IF = ('NULL', 'null')\n",
    "    EMPTY_FIELD_AS_NULL = TRUE;\n",
    "\"\"\").collect()\n",
    "\n",
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE STAGE azure_sales_stage\n",
    "    URL = 'azure://ittechgeniestorage.blob.core.windows.net/sales-data/'\n",
    "    CREDENTIALS = (\n",
    "        AZURE_SAS_TOKEN = '?sp=racwdl&st=2025-10-22T10:47:09Z&se=2025-10-23T19:02:09Z&spr=https&sv=2024-11-04&sr=c&sig=hEF7601nEZP%2Byvbuk9F2FVtAou%2F3%2BoDvfC3fNQ5fLbs%3D'\n",
    "    )\n",
    "    FILE_FORMAT = csv_sales_format;\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"File format and stage created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121c6c8c-a624-418d-b9fe-89b15670c19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage connection successful! Files found:\n",
      " - azure://ittechgeniestorage.blob.core.windows.net/sales-data/Retail_Sales__500_rows__Preview.csv\n"
     ]
    }
   ],
   "source": [
    "# Test stage connection\n",
    "try:\n",
    "    result = session.sql(\"LIST @azure_sales_stage\").collect()\n",
    "    print(\"Stage connection successful! Files found:\")\n",
    "    for row in result:\n",
    "        print(f\" - {row['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing files: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb937ee-7f27-4f47-9046-4c918b594cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sales table created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create raw table schema matching your CSV\n",
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE raw_sales_data (\n",
    "    OrderID STRING,\n",
    "    OrderDate DATE,\n",
    "    MonthOfSale STRING,\n",
    "    CustomerID STRING,\n",
    "    CustomerName STRING,\n",
    "    Country STRING,\n",
    "    Region STRING,\n",
    "    City STRING,\n",
    "    Category STRING,\n",
    "    Subcategory STRING,\n",
    "    Quantity INTEGER,\n",
    "    Discount NUMBER(10,2),\n",
    "    Sales NUMBER(10,2),\n",
    "    Profit NUMBER(10,2),\n",
    "    FileName STRING,\n",
    "    LoadTimestamp TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"Raw sales table created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb8dd0b-1943-422d-b0c7-ce2dc6977998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion completed: 25 rows loaded\n"
     ]
    }
   ],
   "source": [
    "# Ingest data from Azure to Snowflake\n",
    "copy_result = session.sql(\"\"\"\n",
    "COPY INTO raw_sales_data (\n",
    "    OrderID, OrderDate, MonthOfSale, CustomerID, CustomerName, \n",
    "    Country, Region, City, Category, Subcategory, \n",
    "    Quantity, Discount, Sales, Profit, FileName\n",
    ")\n",
    "FROM (\n",
    "    SELECT \n",
    "        $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14,\n",
    "        METADATA$FILENAME\n",
    "    FROM @azure_sales_stage/Retail_Sales__500_rows__Preview.csv\n",
    ")\n",
    "FILE_FORMAT = (FORMAT_NAME = csv_sales_format)\n",
    "ON_ERROR = 'CONTINUE';\n",
    "\"\"\").collect()\n",
    "\n",
    "print(f\"Data ingestion completed: {copy_result[0]['rows_loaded']} rows loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e4277b-1c5f-485f-9def-cf89d3894167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in raw table: 25\n",
      "Sample raw data:\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDERID\"     |\"ORDERDATE\"  |\"MONTHOFSALE\"  |\"CUSTOMERID\"  |\"CUSTOMERNAME\"  |\"COUNTRY\"  |\"REGION\"  |\"CITY\"   |\"CATEGORY\"       |\"SUBCATEGORY\"  |\"QUANTITY\"  |\"DISCOUNT\"  |\"SALES\"   |\"PROFIT\"  |\"FILENAME\"                           |\"LOADTIMESTAMP\"             |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|ORD-5F8D6F0C  |2024-10-08   |2024-10        |CUST1000      |Ananya Sharma   |India      |South     |Mumbai   |Office Supplies  |Paper          |9           |0.00        |2700.00   |780.43    |Retail_Sales__500_rows__Preview.csv  |2025-10-22 04:29:36.850000  |\n",
      "|ORD-BF0078E4  |2024-08-11   |2024-08        |CUST1001      |Aarav Iyer      |India      |Central   |Lucknow  |Technology       |Networking     |4           |0.15        |27200.00  |4135.60   |Retail_Sales__500_rows__Preview.csv  |2025-10-22 04:29:36.850000  |\n",
      "|ORD-86CD58A3  |2024-06-12   |2024-06        |CUST1002      |Arjun Sharma    |USA        |East      |Kolkata  |Furniture        |Tables         |4           |0.10        |31500.00  |5676.96   |Retail_Sales__500_rows__Preview.csv  |2025-10-22 04:29:36.850000  |\n",
      "|ORD-FB0CD2D9  |2024-12-18   |2024-12        |CUST1003      |Ananya Das      |India      |North     |Kolkata  |Office Supplies  |Appliances     |9           |0.00        |36000.00  |11783.22  |Retail_Sales__500_rows__Preview.csv  |2025-10-22 04:29:36.850000  |\n",
      "|ORD-EF35596B  |2024-10-27   |2024-10        |CUST1004      |Ishaan Bhat     |UK         |Central   |Chennai  |Furniture        |Storage        |4           |0.00        |24000.00  |4189.98   |Retail_Sales__500_rows__Preview.csv  |2025-10-22 04:29:36.850000  |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify raw data\n",
    "result = session.sql(\"SELECT COUNT(*) as total_rows FROM raw_sales_data\").collect()\n",
    "print(f\"Total rows in raw table: {result[0]['TOTAL_ROWS']}\")\n",
    "\n",
    "print(\"Sample raw data:\")\n",
    "session.sql(\"SELECT * FROM raw_sales_data LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8066a1dc-0208-40c3-9eed-5703fba59191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean sales data table created\n"
     ]
    }
   ],
   "source": [
    "# Switch to clean schema and create clean data table\n",
    "session.sql(\"USE SCHEMA CLEAN_DATA\").collect()\n",
    "\n",
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE clean_sales_data AS\n",
    "SELECT \n",
    "    OrderID,\n",
    "    OrderDate,\n",
    "    MonthOfSale,\n",
    "    CustomerID,\n",
    "    CustomerName,\n",
    "    Country,\n",
    "    Region,\n",
    "    City,\n",
    "    Category,\n",
    "    Subcategory,\n",
    "    Quantity,\n",
    "    Discount,\n",
    "    Sales,\n",
    "    Profit,\n",
    "    -- Data validation and calculations\n",
    "    CASE WHEN Sales != Quantity * (Sales/NULLIF(Quantity,0)) THEN Sales ELSE Sales END AS ValidatedSales,\n",
    "    -- Date parts for analysis\n",
    "    YEAR(OrderDate) AS OrderYear,\n",
    "    MONTH(OrderDate) AS OrderMonth,\n",
    "    QUARTER(OrderDate) AS OrderQuarter,\n",
    "    -- Business metrics\n",
    "    Sales * Discount AS DiscountAmount,\n",
    "    Profit / NULLIF(Sales, 0) AS ProfitMargin,\n",
    "    LoadTimestamp\n",
    "FROM ITTG_SALES_DB.RAW_DATA.raw_sales_data\n",
    "WHERE OrderDate IS NOT NULL AND Sales > 0;\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"Clean sales data table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bd0fd6c-6dec-40f0-be35-8a2e107003b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated views created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create aggregated views for Power BI\n",
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW sales_summary_monthly AS\n",
    "SELECT\n",
    "    Region,\n",
    "    Category,\n",
    "    OrderYear,\n",
    "    OrderMonth,\n",
    "    COUNT(*) AS TotalOrders,\n",
    "    SUM(Sales) AS TotalSales,\n",
    "    SUM(Profit) AS TotalProfit,\n",
    "    AVG(Sales) AS AvgOrderValue,\n",
    "    SUM(Quantity) AS TotalQuantity,\n",
    "    COUNT(DISTINCT CustomerID) AS UniqueCustomers\n",
    "FROM clean_sales_data\n",
    "GROUP BY Region, Category, OrderYear, OrderMonth\n",
    "ORDER BY OrderYear, OrderMonth, Region;\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"Aggregated views created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a38289b-d91e-48df-a23b-f778a18a3c61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power BI view created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create Power BI optimized view\n",
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE VIEW vw_powerbi_sales_dashboard AS\n",
    "SELECT \n",
    "    cs.*,\n",
    "    sm.TotalSales AS RegionMonthlySales,\n",
    "    sm.TotalProfit AS RegionMonthlyProfit,\n",
    "    sm.UniqueCustomers AS RegionMonthlyCustomers\n",
    "FROM clean_sales_data cs\n",
    "LEFT JOIN sales_summary_monthly sm \n",
    "    ON cs.Region = sm.Region \n",
    "    AND cs.Category = sm.Category\n",
    "    AND cs.OrderYear = sm.OrderYear \n",
    "    AND cs.OrderMonth = sm.OrderMonth;\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"Power BI view created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ede2a3-6369-4851-9a25-e35667283a44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Check Results:\n",
      "Error in check SELECT COUNT(*) AS total_rows FROM raw_sales_data: (1304): 01bfe111-0001-6723-000c-5afa000343a2: 002003 (42S02): SQL compilation error:\n",
      "Object 'RAW_SALES_DATA' does not exist or not authorized.\n",
      "SELECT COUNT(*) AS clean_rows FROM clean_sales_data: Row(CLEAN_ROWS=25)\n",
      "SELECT COUNT(DISTINCT Region) AS region_count FROM clean_sales_data: Row(REGION_COUNT=5)\n",
      "SELECT MIN(OrderDate) AS earliest_date, MAX(OrderDate) AS latest_date FROM clean_sales_data: Row(EARLIEST_DATE=datetime.date(2024, 2, 27), LATEST_DATE=datetime.date(2025, 9, 15))\n",
      "SELECT SUM(Sales) AS total_sales, SUM(Profit) AS total_profit FROM clean_sales_data: Row(TOTAL_SALES=Decimal('1066026.50'), TOTAL_PROFIT=Decimal('180647.47'))\n"
     ]
    }
   ],
   "source": [
    "# Data quality checks\n",
    "checks = [\n",
    "    \"SELECT COUNT(*) AS total_rows FROM raw_sales_data\",\n",
    "    \"SELECT COUNT(*) AS clean_rows FROM clean_sales_data\", \n",
    "    \"SELECT COUNT(DISTINCT Region) AS region_count FROM clean_sales_data\",\n",
    "    \"SELECT MIN(OrderDate) AS earliest_date, MAX(OrderDate) AS latest_date FROM clean_sales_data\",\n",
    "    \"SELECT SUM(Sales) AS total_sales, SUM(Profit) AS total_profit FROM clean_sales_data\"\n",
    "]\n",
    "\n",
    "print(\"Data Quality Check Results:\")\n",
    "for check in checks:\n",
    "    try:\n",
    "        result = session.sql(check).collect()\n",
    "        print(f\"{check}: {result[0]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in check {check}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8bab3a7-84f7-4208-a01e-bd6486ed93a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final verification - Sample from Power BI view:\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"ORDERID\"     |\"ORDERDATE\"  |\"MONTHOFSALE\"  |\"CUSTOMERID\"  |\"CUSTOMERNAME\"  |\"COUNTRY\"  |\"REGION\"  |\"CITY\"   |\"CATEGORY\"       |\"SUBCATEGORY\"  |\"QUANTITY\"  |\"DISCOUNT\"  |\"SALES\"   |\"PROFIT\"  |\"VALIDATEDSALES\"  |\"ORDERYEAR\"  |\"ORDERMONTH\"  |\"ORDERQUARTER\"  |\"DISCOUNTAMOUNT\"  |\"PROFITMARGIN\"  |\"LOADTIMESTAMP\"             |\"REGIONMONTHLYSALES\"  |\"REGIONMONTHLYPROFIT\"  |\"REGIONMONTHLYCUSTOMERS\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|ORD-86CD58A3  |2024-06-12   |2024-06        |CUST1002      |Arjun Sharma    |USA        |East      |Kolkata  |Furniture        |Tables         |4           |0.10        |31500.00  |5676.96   |31500.00          |2024         |6             |2               |3150.0000         |0.18022095      |2025-10-22 04:29:36.850000  |31500.00              |5676.96                |1                         |\n",
      "|ORD-BF0078E4  |2024-08-11   |2024-08        |CUST1001      |Aarav Iyer      |India      |Central   |Lucknow  |Technology       |Networking     |4           |0.15        |27200.00  |4135.60   |27200.00          |2024         |8             |3               |4080.0000         |0.15204412      |2025-10-22 04:29:36.850000  |27200.00              |4135.60                |1                         |\n",
      "|ORD-EF35596B  |2024-10-27   |2024-10        |CUST1004      |Ishaan Bhat     |UK         |Central   |Chennai  |Furniture        |Storage        |4           |0.00        |24000.00  |4189.98   |24000.00          |2024         |10            |4               |0.0000            |0.17458250      |2025-10-22 04:29:36.850000  |24000.00              |4189.98                |1                         |\n",
      "|ORD-5F8D6F0C  |2024-10-08   |2024-10        |CUST1000      |Ananya Sharma   |India      |South     |Mumbai   |Office Supplies  |Paper          |9           |0.00        |2700.00   |780.43    |2700.00           |2024         |10            |4               |0.0000            |0.28904815      |2025-10-22 04:29:36.850000  |2700.00               |780.43                 |1                         |\n",
      "|ORD-FB0CD2D9  |2024-12-18   |2024-12        |CUST1003      |Ananya Das      |India      |North     |Kolkata  |Office Supplies  |Appliances     |9           |0.00        |36000.00  |11783.22  |36000.00          |2024         |12            |4               |0.0000            |0.32731167      |2025-10-22 04:29:36.850000  |36000.00              |11783.22               |1                         |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Summary statistics:\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "|\"TOTAL_RECORDS\"  |\"UNIQUE_CUSTOMERS\"  |\"REGIONS_COVERED\"  |\"TOTAL_SALES\"  |\"TOTAL_PROFIT\"  |\"AVG_PROFIT_MARGIN\"  |\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "|25               |25                  |5                  |1066026.50     |180647.47       |0.216975929200       |\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final verification and sample data\n",
    "print(\"Final verification - Sample from Power BI view:\")\n",
    "session.sql(\"SELECT * FROM vw_powerbi_sales_dashboard LIMIT 5\").show()\n",
    "\n",
    "print(\"Summary statistics:\")\n",
    "session.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) AS total_records,\n",
    "    COUNT(DISTINCT CustomerID) AS unique_customers,\n",
    "    COUNT(DISTINCT Region) AS regions_covered,\n",
    "    SUM(Sales) AS total_sales,\n",
    "    SUM(Profit) AS total_profit,\n",
    "    AVG(ProfitMargin) AS avg_profit_margin\n",
    "FROM vw_powerbi_sales_dashboard\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710e258e-ee32-40c4-ad7a-0e9a179bce8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowpark session closed\n",
      "Pipeline execution completed successfully!\n",
      "\n",
      "Next steps: Connect Power BI to Snowflake using:\n",
      "Database: ITTG_SALES_DB\n",
      "Schema: CLEAN_DATA\n",
      "View: VW_POWERBI_SALES_DASHBOARD\n"
     ]
    }
   ],
   "source": [
    "# Close session\n",
    "session.close()\n",
    "print(\"Snowpark session closed\")\n",
    "print(\"Pipeline execution completed successfully!\")\n",
    "print(\"\\nNext steps: Connect Power BI to Snowflake using:\")\n",
    "print(\"Database: ITTG_SALES_DB\")\n",
    "print(\"Schema: CLEAN_DATA\") \n",
    "print(\"View: VW_POWERBI_SALES_DASHBOARD\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "azure-connection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
