{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf20fc8-ac04-4550-a70a-b12f32d59326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-connector-python in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (3.18.0)\nRequirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from snowflake-connector-python) (1.5.1)\nRequirement already satisfied: boto3>=1.24 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python) (1.34.69)\nRequirement already satisfied: botocore>=1.24 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python) (1.34.69)\nRequirement already satisfied: cffi<2.0.0,>=1.9 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python) (1.16.0)\nRequirement already satisfied: cryptography>=3.1.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from snowflake-connector-python) (46.0.0)\nRequirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from snowflake-connector-python) (25.3.0)\nRequirement already satisfied: pyjwt<3.0.0 in /usr/lib/python3/dist-packages (from snowflake-connector-python) (2.7.0)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python) (2024.1)\nRequirement already satisfied: requests<3.0.0 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python) (2.32.2)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python) (24.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python) (2024.6.2)\nRequirement already satisfied: typing_extensions<5,>=4.3 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python) (4.11.0)\nRequirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.12/dist-packages (from snowflake-connector-python) (3.15.4)\nRequirement already satisfied: sortedcontainers>=2.4.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from snowflake-connector-python) (2.4.0)\nRequirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /databricks/python3/lib/python3.12/site-packages (from snowflake-connector-python) (3.10.0)\nRequirement already satisfied: tomlkit in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from snowflake-connector-python) (0.13.3)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\nRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.12/site-packages (from boto3>=1.24->snowflake-connector-python) (0.10.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python) (2.9.0.post0)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /databricks/python3/lib/python3.12/site-packages (from botocore>=1.24->snowflake-connector-python) (1.26.16)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f590f38a-6ebd-4e4c-82a7-e6d87bcbd2d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34830aef-d096-489a-b6fc-98029f0b2fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake Configuration:\nURL: tymhdzv-pz92491.snowflakecomputing.com\nUser: RUTHRA\nDatabase: SNOWFLAKE_SAMPLE_DATA\nSchema: TPCH_SF1\nWarehouse: COMPUTE_WH\n"
     ]
    }
   ],
   "source": [
    "sfOptions = {\n",
    "    \"sfUrl\": \"tymhdzv-pz92491.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"RUTHRA\",\n",
    "    \"sfPassword\": \"Ruthra#978Snowflake\",\n",
    "    \"sfDatabase\": \"SNOWFLAKE_SAMPLE_DATA\", \n",
    "    \"sfSchema\": \"TPCH_SF1\",  \n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",  \n",
    "    \"sfRole\": \"ACCOUNTADMIN\"\n",
    "}\n",
    "\n",
    "# Print configuration (masking password for security)\n",
    "print(\"Snowflake Configuration:\")\n",
    "print(f\"URL: {sfOptions['sfUrl']}\")\n",
    "print(f\"User: {sfOptions['sfUser']}\")\n",
    "print(f\"Database: {sfOptions['sfDatabase']}\")\n",
    "print(f\"Schema: {sfOptions['sfSchema']}\")\n",
    "print(f\"Warehouse: {sfOptions['sfWarehouse']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001d48a0-c3e3-4582-ba93-0b198115c5ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Snowflake connection...\n Connection successful!\n Connection failed: current_time\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_snowflake_connection(sf_options):\n",
    "    try:\n",
    "        print(\"Testing Snowflake connection...\")\n",
    "        \n",
    "        # Simple test query\n",
    "        test_df = spark.read \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sf_options) \\\n",
    "            .option(\"query\", \"SELECT CURRENT_TIMESTAMP() as current_time, CURRENT_VERSION() as version\") \\\n",
    "            .load()\n",
    "        \n",
    "        result = test_df.collect()[0]\n",
    "        print(f\" Connection successful!\")\n",
    "        print(f\"   Current Time: {result['current_time']}\")\n",
    "        print(f\"   Snowflake Version: {result['version']}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Connection failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Execute connection test\n",
    "test_snowflake_connection(sfOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3822a815-1c58-4153-97a1-0e7c2b5b86b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Available Tables in TPCH_SF1 schema:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>TABLE_NAME</th><th>TABLE_TYPE</th><th>ROW_COUNT</th></tr></thead><tbody><tr><td>CUSTOMER</td><td>BASE TABLE</td><td>150000</td></tr><tr><td>LINEITEM</td><td>BASE TABLE</td><td>6001215</td></tr><tr><td>NATION</td><td>BASE TABLE</td><td>25</td></tr><tr><td>ORDERS</td><td>BASE TABLE</td><td>1500000</td></tr><tr><td>PART</td><td>BASE TABLE</td><td>200000</td></tr><tr><td>PARTSUPP</td><td>BASE TABLE</td><td>800000</td></tr><tr><td>REGION</td><td>BASE TABLE</td><td>5</td></tr><tr><td>SUPPLIER</td><td>BASE TABLE</td><td>10000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "CUSTOMER",
         "BASE TABLE",
         "150000"
        ],
        [
         "LINEITEM",
         "BASE TABLE",
         "6001215"
        ],
        [
         "NATION",
         "BASE TABLE",
         "25"
        ],
        [
         "ORDERS",
         "BASE TABLE",
         "1500000"
        ],
        [
         "PART",
         "BASE TABLE",
         "200000"
        ],
        [
         "PARTSUPP",
         "BASE TABLE",
         "800000"
        ],
        [
         "REGION",
         "BASE TABLE",
         "5"
        ],
        [
         "SUPPLIER",
         "BASE TABLE",
         "10000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "TABLE_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TABLE_TYPE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ROW_COUNT",
         "type": "\"decimal(38,0)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[TABLE_NAME: string, TABLE_TYPE: string, ROW_COUNT: decimal(38,0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_snowflake_tables(sf_options):\n",
    "    try:\n",
    "        tables_df = spark.read \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sf_options) \\\n",
    "            .option(\"query\", \"\"\"\n",
    "                SELECT TABLE_NAME, TABLE_TYPE, ROW_COUNT \n",
    "                FROM INFORMATION_SCHEMA.TABLES \n",
    "                WHERE TABLE_SCHEMA = 'TPCH_SF1'\n",
    "                ORDER BY TABLE_NAME\n",
    "            \"\"\") \\\n",
    "            .load()\n",
    "        \n",
    "        print(\" Available Tables in TPCH_SF1 schema:\")\n",
    "        display(tables_df)\n",
    "        return tables_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing tables: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "list_snowflake_tables(sfOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "844a16d6-7b50-4953-b511-c9194ebdb0f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDC65 Reading customer data...\n Successfully loaded 150000 customer records\n Schema:\nroot\n |-- C_CUSTKEY: decimal(38,0) (nullable = false)\n |-- C_NAME: string (nullable = false)\n |-- C_ADDRESS: string (nullable = false)\n |-- C_NATIONKEY: decimal(38,0) (nullable = false)\n |-- C_PHONE: string (nullable = false)\n |-- C_ACCTBAL: decimal(12,2) (nullable = false)\n |-- C_MKTSEGMENT: string (nullable = true)\n |-- C_COMMENT: string (nullable = true)\n\n\n Sample Customer Data (first 10 rows):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>C_CUSTKEY</th><th>C_NAME</th><th>C_ADDRESS</th><th>C_NATIONKEY</th><th>C_PHONE</th><th>C_ACCTBAL</th><th>C_MKTSEGMENT</th><th>C_COMMENT</th></tr></thead><tbody><tr><td>60001</td><td>Customer#000060001</td><td>9Ii4zQn9cX</td><td>14</td><td>24-678-784-9652</td><td>9957.56</td><td>HOUSEHOLD</td><td>l theodolites boost slyly at the platelets: permanently ironic packages wake slyly pend</td></tr><tr><td>60002</td><td>Customer#000060002</td><td>ThGBMjDwKzkoOxhz</td><td>15</td><td>25-782-500-8435</td><td>742.46</td><td>BUILDING</td><td> beans. fluffily regular packages</td></tr><tr><td>60003</td><td>Customer#000060003</td><td>Ed hbPtTXMTAsgGhCr4HuTzK,Md2</td><td>16</td><td>26-859-847-7640</td><td>2526.92</td><td>BUILDING</td><td>fully pending deposits sleep quickly. blithely unusual accounts across the blithely bold requests are quickly</td></tr><tr><td>60004</td><td>Customer#000060004</td><td>NivCT2RVaavl,yUnKwBjDyMvB42WayXCnky</td><td>10</td><td>20-573-674-7999</td><td>7975.22</td><td>AUTOMOBILE</td><td> furiously above the ironic packages. slyly brave ideas boost. final platelets detect according to the ironi</td></tr><tr><td>60005</td><td>Customer#000060005</td><td>1F3KM3ccEXEtI, B22XmCMOWJMl</td><td>12</td><td>22-741-208-1316</td><td>2504.74</td><td>MACHINERY</td><td>express instructions sleep quickly. ironic braids cajole furiously fluffily p</td></tr><tr><td>60006</td><td>Customer#000060006</td><td>3isiXW651fa8p </td><td>22</td><td>32-618-195-8029</td><td>9051.40</td><td>MACHINERY</td><td> carefully quickly even theodolites. boldly </td></tr><tr><td>60007</td><td>Customer#000060007</td><td>sp6KJmx,TiSWbMPvhkQwFwTuhSi4a5OLNImpcGI</td><td>12</td><td>22-491-919-9470</td><td>6017.17</td><td>FURNITURE</td><td>bold packages. regular sheaves mold. blit</td></tr><tr><td>60008</td><td>Customer#000060008</td><td>3VteHZYOfbgQioA96tUeL0R7i</td><td>2</td><td>12-693-562-7122</td><td>5621.44</td><td>AUTOMOBILE</td><td>nal courts. carefully regular Tiresias lose quickly unusual packages. regular, bold i</td></tr><tr><td>60009</td><td>Customer#000060009</td><td>S60sNpR6wnacPBLeOxjxhvehf</td><td>9</td><td>19-578-776-2699</td><td>9548.01</td><td>FURNITURE</td><td>efully even dependencies haggle furiously along the express packages. final requests boost</td></tr><tr><td>60010</td><td>Customer#000060010</td><td>c4vEEaV1tdqLdw2oVuXp BN</td><td>21</td><td>31-677-809-6961</td><td>3497.91</td><td>HOUSEHOLD</td><td>fter the quickly silent requests. slyly special theodolites along the even, even requests boos</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "60001",
         "Customer#000060001",
         "9Ii4zQn9cX",
         "14",
         "24-678-784-9652",
         "9957.56",
         "HOUSEHOLD",
         "l theodolites boost slyly at the platelets: permanently ironic packages wake slyly pend"
        ],
        [
         "60002",
         "Customer#000060002",
         "ThGBMjDwKzkoOxhz",
         "15",
         "25-782-500-8435",
         "742.46",
         "BUILDING",
         " beans. fluffily regular packages"
        ],
        [
         "60003",
         "Customer#000060003",
         "Ed hbPtTXMTAsgGhCr4HuTzK,Md2",
         "16",
         "26-859-847-7640",
         "2526.92",
         "BUILDING",
         "fully pending deposits sleep quickly. blithely unusual accounts across the blithely bold requests are quickly"
        ],
        [
         "60004",
         "Customer#000060004",
         "NivCT2RVaavl,yUnKwBjDyMvB42WayXCnky",
         "10",
         "20-573-674-7999",
         "7975.22",
         "AUTOMOBILE",
         " furiously above the ironic packages. slyly brave ideas boost. final platelets detect according to the ironi"
        ],
        [
         "60005",
         "Customer#000060005",
         "1F3KM3ccEXEtI, B22XmCMOWJMl",
         "12",
         "22-741-208-1316",
         "2504.74",
         "MACHINERY",
         "express instructions sleep quickly. ironic braids cajole furiously fluffily p"
        ],
        [
         "60006",
         "Customer#000060006",
         "3isiXW651fa8p ",
         "22",
         "32-618-195-8029",
         "9051.40",
         "MACHINERY",
         " carefully quickly even theodolites. boldly "
        ],
        [
         "60007",
         "Customer#000060007",
         "sp6KJmx,TiSWbMPvhkQwFwTuhSi4a5OLNImpcGI",
         "12",
         "22-491-919-9470",
         "6017.17",
         "FURNITURE",
         "bold packages. regular sheaves mold. blit"
        ],
        [
         "60008",
         "Customer#000060008",
         "3VteHZYOfbgQioA96tUeL0R7i",
         "2",
         "12-693-562-7122",
         "5621.44",
         "AUTOMOBILE",
         "nal courts. carefully regular Tiresias lose quickly unusual packages. regular, bold i"
        ],
        [
         "60009",
         "Customer#000060009",
         "S60sNpR6wnacPBLeOxjxhvehf",
         "9",
         "19-578-776-2699",
         "9548.01",
         "FURNITURE",
         "efully even dependencies haggle furiously along the express packages. final requests boost"
        ],
        [
         "60010",
         "Customer#000060010",
         "c4vEEaV1tdqLdw2oVuXp BN",
         "21",
         "31-677-809-6961",
         "3497.91",
         "HOUSEHOLD",
         "fter the quickly silent requests. slyly special theodolites along the even, even requests boos"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "C_CUSTKEY",
         "type": "\"decimal(38,0)\""
        },
        {
         "metadata": "{}",
         "name": "C_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "C_ADDRESS",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "C_NATIONKEY",
         "type": "\"decimal(38,0)\""
        },
        {
         "metadata": "{}",
         "name": "C_PHONE",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "C_ACCTBAL",
         "type": "\"decimal(12,2)\""
        },
        {
         "metadata": "{}",
         "name": "C_MKTSEGMENT",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "C_COMMENT",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_customer_data(sf_options):\n",
    "    try:\n",
    "        print(\"\uD83D\uDC65 Reading customer data...\")\n",
    "        \n",
    "        customer_df = spark.read \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sf_options) \\\n",
    "            .option(\"dbtable\", \"CUSTOMER\") \\\n",
    "            .load()\n",
    "            # .limit(1000)  # Optional: limit rows for testing\n",
    "        \n",
    "        print(f\" Successfully loaded {customer_df.count()} customer records\")\n",
    "        print(\" Schema:\")\n",
    "        customer_df.printSchema()\n",
    "        \n",
    "        return customer_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error reading customer data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "customer_df = read_customer_data(sfOptions)\n",
    "\n",
    "# Display sample data\n",
    "if customer_df:\n",
    "    print(\"\\n Sample Customer Data (first 10 rows):\")\n",
    "    display(customer_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc349604-9963-4c42-9dfa-958138fde3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Orders count: 1500000\n Nations count: 25\n\n Sample Orders Data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>O_ORDERKEY</th><th>O_CUSTKEY</th><th>O_ORDERSTATUS</th><th>O_TOTALPRICE</th><th>O_ORDERDATE</th><th>O_ORDERPRIORITY</th><th>O_CLERK</th><th>O_SHIPPRIORITY</th><th>O_COMMENT</th></tr></thead><tbody><tr><td>3000001</td><td>145618</td><td>F</td><td>30175.88</td><td>1992-12-17</td><td>4-NOT SPECIFIED</td><td>Clerk#000000141</td><td>0</td><td>l packages. furiously careful instructions grow furi</td></tr><tr><td>3000002</td><td>1481</td><td>O</td><td>297999.63</td><td>1995-07-28</td><td>1-URGENT</td><td>Clerk#000000547</td><td>0</td><td>carefully unusual dependencie</td></tr><tr><td>3000003</td><td>127432</td><td>O</td><td>345438.38</td><td>1997-11-04</td><td>5-LOW</td><td>Clerk#000000488</td><td>0</td><td>n packages boost slyly bold deposits. deposits around the ironic th</td></tr><tr><td>3000004</td><td>47423</td><td>O</td><td>135965.53</td><td>1996-06-13</td><td>4-NOT SPECIFIED</td><td>Clerk#000000004</td><td>0</td><td>nts wake carefully final decoys. quickly final accounts wake </td></tr><tr><td>3000005</td><td>84973</td><td>F</td><td>209937.09</td><td>1992-09-12</td><td>5-LOW</td><td>Clerk#000000030</td><td>0</td><td>yly after the quickly unusual ide</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "3000001",
         "145618",
         "F",
         "30175.88",
         "1992-12-17",
         "4-NOT SPECIFIED",
         "Clerk#000000141",
         "0",
         "l packages. furiously careful instructions grow furi"
        ],
        [
         "3000002",
         "1481",
         "O",
         "297999.63",
         "1995-07-28",
         "1-URGENT",
         "Clerk#000000547",
         "0",
         "carefully unusual dependencie"
        ],
        [
         "3000003",
         "127432",
         "O",
         "345438.38",
         "1997-11-04",
         "5-LOW",
         "Clerk#000000488",
         "0",
         "n packages boost slyly bold deposits. deposits around the ironic th"
        ],
        [
         "3000004",
         "47423",
         "O",
         "135965.53",
         "1996-06-13",
         "4-NOT SPECIFIED",
         "Clerk#000000004",
         "0",
         "nts wake carefully final decoys. quickly final accounts wake "
        ],
        [
         "3000005",
         "84973",
         "F",
         "209937.09",
         "1992-09-12",
         "5-LOW",
         "Clerk#000000030",
         "0",
         "yly after the quickly unusual ide"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "O_ORDERKEY",
         "type": "\"decimal(38,0)\""
        },
        {
         "metadata": "{}",
         "name": "O_CUSTKEY",
         "type": "\"decimal(38,0)\""
        },
        {
         "metadata": "{}",
         "name": "O_ORDERSTATUS",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "O_TOTALPRICE",
         "type": "\"decimal(12,2)\""
        },
        {
         "metadata": "{}",
         "name": "O_ORDERDATE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "O_ORDERPRIORITY",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "O_CLERK",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "O_SHIPPRIORITY",
         "type": "\"decimal(38,0)\""
        },
        {
         "metadata": "{}",
         "name": "O_COMMENT",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Sample Nation Data:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>N_NATIONKEY</th><th>N_NAME</th><th>N_REGIONKEY</th><th>N_COMMENT</th></tr></thead><tbody><tr><td>0</td><td>ALGERIA</td><td>0</td><td> haggle. carefully final deposits detect slyly agai</td></tr><tr><td>1</td><td>ARGENTINA</td><td>1</td><td>al foxes promise slyly according to the regular accounts. bold requests alon</td></tr><tr><td>2</td><td>BRAZIL</td><td>1</td><td>y alongside of the pending deposits. carefully special packages are about the ironic forges. slyly special </td></tr><tr><td>3</td><td>CANADA</td><td>1</td><td>eas hang ironic, silent packages. slyly regular packages are furiously over the tithes. fluffily bold</td></tr><tr><td>4</td><td>EGYPT</td><td>4</td><td>y above the carefully unusual theodolites. final dugouts are quickly across the furiously regular d</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "0",
         "ALGERIA",
         "0",
         " haggle. carefully final deposits detect slyly agai"
        ],
        [
         "1",
         "ARGENTINA",
         "1",
         "al foxes promise slyly according to the regular accounts. bold requests alon"
        ],
        [
         "2",
         "BRAZIL",
         "1",
         "y alongside of the pending deposits. carefully special packages are about the ironic forges. slyly special "
        ],
        [
         "3",
         "CANADA",
         "1",
         "eas hang ironic, silent packages. slyly regular packages are furiously over the tithes. fluffily bold"
        ],
        [
         "4",
         "EGYPT",
         "4",
         "y above the carefully unusual theodolites. final dugouts are quickly across the furiously regular d"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "N_NATIONKEY",
         "type": "\"decimal(38,0)\""
        },
        {
         "metadata": "{}",
         "name": "N_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "N_REGIONKEY",
         "type": "\"decimal(38,0)\""
        },
        {
         "metadata": "{}",
         "name": "N_COMMENT",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_multiple_tables(sf_options):\n",
    "    try:\n",
    "        # Read orders data\n",
    "        orders_df = spark.read \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sf_options) \\\n",
    "            .option(\"dbtable\", \"ORDERS\") \\\n",
    "            .load()\n",
    "        \n",
    "        # Read nation data\n",
    "        nation_df = spark.read \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sf_options) \\\n",
    "            .option(\"dbtable\", \"NATION\") \\\n",
    "            .load()\n",
    "        \n",
    "        print(f\" Orders count: {orders_df.count()}\")\n",
    "        print(f\" Nations count: {nation_df.count()}\")\n",
    "        \n",
    "        return orders_df, nation_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error reading tables: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "orders_df, nation_df = read_multiple_tables(sfOptions)\n",
    "\n",
    "# Display sample data\n",
    "if orders_df and nation_df:\n",
    "    print(\"\\n Sample Orders Data:\")\n",
    "    display(orders_df.limit(5))\n",
    "    \n",
    "    print(\"\\n Sample Nation Data:\")\n",
    "    display(nation_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da62dc65-91de-4276-9cc7-2ebf99c86e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD17 Executing complex query with joins...\n Complex query executed successfully. Returned 20 rows\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>C_CUSTKEY</th><th>C_NAME</th><th>C_NATIONKEY</th><th>NATION_NAME</th><th>ORDER_COUNT</th><th>TOTAL_SPENT</th></tr></thead><tbody><tr><td>143500</td><td>Customer#000143500</td><td>10</td><td>IRAN</td><td>39</td><td>7012696.48</td></tr><tr><td>95257</td><td>Customer#000095257</td><td>2</td><td>BRAZIL</td><td>36</td><td>6563511.23</td></tr><tr><td>87115</td><td>Customer#000087115</td><td>14</td><td>KENYA</td><td>34</td><td>6457526.26</td></tr><tr><td>131113</td><td>Customer#000131113</td><td>5</td><td>ETHIOPIA</td><td>37</td><td>6311428.86</td></tr><tr><td>103834</td><td>Customer#000103834</td><td>11</td><td>IRAQ</td><td>31</td><td>6306524.23</td></tr><tr><td>134380</td><td>Customer#000134380</td><td>0</td><td>ALGERIA</td><td>37</td><td>6291610.15</td></tr><tr><td>69682</td><td>Customer#000069682</td><td>16</td><td>MOZAMBIQUE</td><td>39</td><td>6287149.42</td></tr><tr><td>102022</td><td>Customer#000102022</td><td>9</td><td>INDONESIA</td><td>41</td><td>6273788.41</td></tr><tr><td>98587</td><td>Customer#000098587</td><td>18</td><td>CHINA</td><td>37</td><td>6265089.35</td></tr><tr><td>85102</td><td>Customer#000085102</td><td>15</td><td>MOROCCO</td><td>34</td><td>6135483.63</td></tr><tr><td>64660</td><td>Customer#000064660</td><td>16</td><td>MOZAMBIQUE</td><td>31</td><td>6125248.52</td></tr><tr><td>56317</td><td>Customer#000056317</td><td>7</td><td>GERMANY</td><td>34</td><td>6117381.16</td></tr><tr><td>127213</td><td>Customer#000127213</td><td>10</td><td>IRAN</td><td>35</td><td>6116235.31</td></tr><tr><td>148750</td><td>Customer#000148750</td><td>18</td><td>CHINA</td><td>37</td><td>6086055.58</td></tr><tr><td>20194</td><td>Customer#000020194</td><td>23</td><td>UNITED KINGDOM</td><td>36</td><td>6079142.09</td></tr><tr><td>79300</td><td>Customer#000079300</td><td>20</td><td>SAUDI ARABIA</td><td>40</td><td>6070254.89</td></tr><tr><td>28180</td><td>Customer#000028180</td><td>6</td><td>FRANCE</td><td>32</td><td>6058487.23</td></tr><tr><td>133069</td><td>Customer#000133069</td><td>6</td><td>FRANCE</td><td>34</td><td>6056425.50</td></tr><tr><td>42205</td><td>Customer#000042205</td><td>22</td><td>RUSSIA</td><td>36</td><td>6041521.63</td></tr><tr><td>35398</td><td>Customer#000035398</td><td>5</td><td>ETHIOPIA</td><td>31</td><td>6024207.39</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "143500",
         "Customer#000143500",
         "10",
         "IRAN",
         "39",
         "7012696.48"
        ],
        [
         "95257",
         "Customer#000095257",
         "2",
         "BRAZIL",
         "36",
         "6563511.23"
        ],
        [
         "87115",
         "Customer#000087115",
         "14",
         "KENYA",
         "34",
         "6457526.26"
        ],
        [
         "131113",
         "Customer#000131113",
         "5",
         "ETHIOPIA",
         "37",
         "6311428.86"
        ],
        [
         "103834",
         "Customer#000103834",
         "11",
         "IRAQ",
         "31",
         "6306524.23"
        ],
        [
         "134380",
         "Customer#000134380",
         "0",
         "ALGERIA",
         "37",
         "6291610.15"
        ],
        [
         "69682",
         "Customer#000069682",
         "16",
         "MOZAMBIQUE",
         "39",
         "6287149.42"
        ],
        [
         "102022",
         "Customer#000102022",
         "9",
         "INDONESIA",
         "41",
         "6273788.41"
        ],
        [
         "98587",
         "Customer#000098587",
         "18",
         "CHINA",
         "37",
         "6265089.35"
        ],
        [
         "85102",
         "Customer#000085102",
         "15",
         "MOROCCO",
         "34",
         "6135483.63"
        ],
        [
         "64660",
         "Customer#000064660",
         "16",
         "MOZAMBIQUE",
         "31",
         "6125248.52"
        ],
        [
         "56317",
         "Customer#000056317",
         "7",
         "GERMANY",
         "34",
         "6117381.16"
        ],
        [
         "127213",
         "Customer#000127213",
         "10",
         "IRAN",
         "35",
         "6116235.31"
        ],
        [
         "148750",
         "Customer#000148750",
         "18",
         "CHINA",
         "37",
         "6086055.58"
        ],
        [
         "20194",
         "Customer#000020194",
         "23",
         "UNITED KINGDOM",
         "36",
         "6079142.09"
        ],
        [
         "79300",
         "Customer#000079300",
         "20",
         "SAUDI ARABIA",
         "40",
         "6070254.89"
        ],
        [
         "28180",
         "Customer#000028180",
         "6",
         "FRANCE",
         "32",
         "6058487.23"
        ],
        [
         "133069",
         "Customer#000133069",
         "6",
         "FRANCE",
         "34",
         "6056425.50"
        ],
        [
         "42205",
         "Customer#000042205",
         "22",
         "RUSSIA",
         "36",
         "6041521.63"
        ],
        [
         "35398",
         "Customer#000035398",
         "5",
         "ETHIOPIA",
         "31",
         "6024207.39"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "C_CUSTKEY",
         "type": "\"decimal(38,0)\""
        },
        {
         "metadata": "{}",
         "name": "C_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "C_NATIONKEY",
         "type": "\"decimal(38,0)\""
        },
        {
         "metadata": "{}",
         "name": "NATION_NAME",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ORDER_COUNT",
         "type": "\"decimal(18,0)\""
        },
        {
         "metadata": "{}",
         "name": "TOTAL_SPENT",
         "type": "\"decimal(24,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def execute_complex_query(sf_options):\n",
    "    try:\n",
    "        print(\"\uD83D\uDD17 Executing complex query with joins...\")\n",
    "        \n",
    "        complex_query = \"\"\"\n",
    "        SELECT \n",
    "            c.C_CUSTKEY,\n",
    "            c.C_NAME,\n",
    "            c.C_NATIONKEY,\n",
    "            n.N_NAME as NATION_NAME,\n",
    "            COUNT(o.O_ORDERKEY) as ORDER_COUNT,\n",
    "            SUM(o.O_TOTALPRICE) as TOTAL_SPENT\n",
    "        FROM CUSTOMER c\n",
    "        JOIN NATION n ON c.C_NATIONKEY = n.N_NATIONKEY\n",
    "        JOIN ORDERS o ON c.C_CUSTKEY = o.O_CUSTKEY\n",
    "        GROUP BY c.C_CUSTKEY, c.C_NAME, c.C_NATIONKEY, n.N_NAME\n",
    "        ORDER BY TOTAL_SPENT DESC\n",
    "        LIMIT 20\n",
    "        \"\"\"\n",
    "        \n",
    "        result_df = spark.read \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sf_options) \\\n",
    "            .option(\"query\", complex_query) \\\n",
    "            .load()\n",
    "        \n",
    "        print(f\" Complex query executed successfully. Returned {result_df.count()} rows\")\n",
    "        display(result_df)\n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error in complex query: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "complex_result = execute_complex_query(sfOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2decbf-7d8e-4ac6-9fac-6e49e4d03479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 Creating sample data and writing to Snowflake...\n Sample DataFrame to write:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>first_name</th><th>last_name</th><th>email</th><th>created_date</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>Johnson</td><td>alice@email.com</td><td>2024-01-15</td></tr><tr><td>2</td><td>Bob</td><td>Smith</td><td>bob@email.com</td><td>2024-01-16</td></tr><tr><td>3</td><td>Charlie</td><td>Brown</td><td>charlie@email.com</td><td>2024-01-17</td></tr><tr><td>4</td><td>Diana</td><td>Prince</td><td>diana@email.com</td><td>2024-01-18</td></tr><tr><td>5</td><td>Edward</td><td>Wilson</td><td>edward@email.com</td><td>2024-01-19</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         "Johnson",
         "alice@email.com",
         "2024-01-15"
        ],
        [
         2,
         "Bob",
         "Smith",
         "bob@email.com",
         "2024-01-16"
        ],
        [
         3,
         "Charlie",
         "Brown",
         "charlie@email.com",
         "2024-01-17"
        ],
        [
         4,
         "Diana",
         "Prince",
         "diana@email.com",
         "2024-01-18"
        ],
        [
         5,
         "Edward",
         "Wilson",
         "edward@email.com",
         "2024-01-19"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "first_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "last_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "email",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "created_date",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error writing to Snowflake: An error occurred while calling o663.save.\n: net.snowflake.client.jdbc.SnowflakeSQLException: SQL execution error: Creating stage on shared database 'SNOWFLAKE_SAMPLE_DATA' is not allowed.\n\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:170)\n\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowException(SnowflakeUtil.java:103)\n\tat net.snowflake.client.core.StmtUtil.pollForOutput(StmtUtil.java:501)\n\tat net.snowflake.client.core.StmtUtil.execute(StmtUtil.java:407)\n\tat net.snowflake.client.core.SFStatement.executeHelper(SFStatement.java:498)\n\tat net.snowflake.client.core.SFStatement.executeQueryInternal(SFStatement.java:215)\n\tat net.snowflake.client.core.SFStatement.executeQuery(SFStatement.java:149)\n\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:785)\n\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:693)\n\tat net.snowflake.client.jdbc.SnowflakeStatementV1.executeQueryInternal(SnowflakeStatementV1.java:296)\n\tat net.snowflake.client.jdbc.SnowflakePreparedStatementV1.executeQuery(SnowflakePreparedStatementV1.java:157)\n\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executePreparedQueryInterruptibly$1(SnowflakeJDBCWrapper.scala:229)\n\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executeInterruptibly$2(SnowflakeJDBCWrapper.scala:274)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:356)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:568)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1171)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:493)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:455)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:310)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:170)\n\t\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowException(SnowflakeUtil.java:103)\n\t\tat net.snowflake.client.core.StmtUtil.pollForOutput(StmtUtil.java:501)\n\t\tat net.snowflake.client.core.StmtUtil.execute(StmtUtil.java:407)\n\t\tat net.snowflake.client.core.SFStatement.executeHelper(SFStatement.java:498)\n\t\tat net.snowflake.client.core.SFStatement.executeQueryInternal(SFStatement.java:215)\n\t\tat net.snowflake.client.core.SFStatement.executeQuery(SFStatement.java:149)\n\t\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:785)\n\t\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:693)\n\t\tat net.snowflake.client.jdbc.SnowflakeStatementV1.executeQueryInternal(SnowflakeStatementV1.java:296)\n\t\tat net.snowflake.client.jdbc.SnowflakePreparedStatementV1.executeQuery(SnowflakePreparedStatementV1.java:157)\n\t\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executePreparedQueryInterruptibly$1(SnowflakeJDBCWrapper.scala:229)\n\t\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executeInterruptibly$2(SnowflakeJDBCWrapper.scala:274)\n\t\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\t\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\t\tat scala.util.Success.map(Try.scala:213)\n\t\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\t\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\t\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\t\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t\t... 1 more\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_and_write_table(sf_options):\n",
    "    try:\n",
    "        print(\"\uD83D\uDD04 Creating sample data and writing to Snowflake...\")\n",
    "        \n",
    "        # Create sample DataFrame\n",
    "        sample_data = [\n",
    "            (1, \"Alice\", \"Johnson\", \"alice@email.com\", \"2024-01-15\"),\n",
    "            (2, \"Bob\", \"Smith\", \"bob@email.com\", \"2024-01-16\"),\n",
    "            (3, \"Charlie\", \"Brown\", \"charlie@email.com\", \"2024-01-17\"),\n",
    "            (4, \"Diana\", \"Prince\", \"diana@email.com\", \"2024-01-18\"),\n",
    "            (5, \"Edward\", \"Wilson\", \"edward@email.com\", \"2024-01-19\")\n",
    "        ]\n",
    "        \n",
    "        columns = [\"id\", \"first_name\", \"last_name\", \"email\", \"created_date\"]\n",
    "        sample_df = spark.createDataFrame(sample_data, columns)\n",
    "        \n",
    "        print(\" Sample DataFrame to write:\")\n",
    "        display(sample_df)\n",
    "        \n",
    "        # Write to Snowflake (create new table)\n",
    "        sample_df.write \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sf_options) \\\n",
    "            .option(\"dbtable\", \"DATABRICKS_SAMPLE\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "        # Options: \"overwrite\", \"append\", \"error\", \"ignore\"\n",
    "        print(\"✅ Successfully wrote data to Snowflake table 'DATABRICKS_SAMPLE'\")\n",
    "        \n",
    "        # Verify the data was written\n",
    "        verify_df = spark.read \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sf_options) \\\n",
    "            .option(\"dbtable\", \"DATABRICKS_SAMPLE\") \\\n",
    "            .load()\n",
    "        \n",
    "        print(\" Verification - Reading back the written data:\")\n",
    "        display(verify_df)\n",
    "        \n",
    "        return sample_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error writing to Snowflake: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "created_df = create_and_write_table(sfOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6462c3-88a4-4c52-88a2-96a031696b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting advanced operations...\n Loaded LINEITEM table with 6001215 rows\nTransformed data (aggregated by ship date):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>L_SHIPDATE</th><th>total_quantity</th><th>avg_price</th><th>transaction_count</th></tr></thead><tbody><tr><td>1998-01-01</td><td>64078.00</td><td>38003.915991</td><td>2537</td></tr><tr><td>1998-01-02</td><td>64562.00</td><td>38277.023458</td><td>2519</td></tr><tr><td>1998-01-03</td><td>64775.00</td><td>38753.180341</td><td>2524</td></tr><tr><td>1998-01-04</td><td>61569.00</td><td>38019.871487</td><td>2435</td></tr><tr><td>1998-01-05</td><td>64343.00</td><td>37913.042836</td><td>2556</td></tr><tr><td>1998-01-06</td><td>62609.00</td><td>38126.159853</td><td>2453</td></tr><tr><td>1998-01-07</td><td>60907.00</td><td>37955.183225</td><td>2409</td></tr><tr><td>1998-01-08</td><td>62822.00</td><td>37974.876689</td><td>2489</td></tr><tr><td>1998-01-09</td><td>62786.00</td><td>37555.015118</td><td>2503</td></tr><tr><td>1998-01-10</td><td>65427.00</td><td>37899.491846</td><td>2606</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1998-01-01",
         "64078.00",
         "38003.915991",
         2537
        ],
        [
         "1998-01-02",
         "64562.00",
         "38277.023458",
         2519
        ],
        [
         "1998-01-03",
         "64775.00",
         "38753.180341",
         2524
        ],
        [
         "1998-01-04",
         "61569.00",
         "38019.871487",
         2435
        ],
        [
         "1998-01-05",
         "64343.00",
         "37913.042836",
         2556
        ],
        [
         "1998-01-06",
         "62609.00",
         "38126.159853",
         2453
        ],
        [
         "1998-01-07",
         "60907.00",
         "37955.183225",
         2409
        ],
        [
         "1998-01-08",
         "62822.00",
         "37974.876689",
         2489
        ],
        [
         "1998-01-09",
         "62786.00",
         "37555.015118",
         2503
        ],
        [
         "1998-01-10",
         "65427.00",
         "37899.491846",
         2606
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "L_SHIPDATE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "total_quantity",
         "type": "\"decimal(22,2)\""
        },
        {
         "metadata": "{}",
         "name": "avg_price",
         "type": "\"decimal(16,6)\""
        },
        {
         "metadata": "{}",
         "name": "transaction_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Error in advanced operations: An error occurred while calling o624.save.\n: net.snowflake.client.jdbc.SnowflakeSQLException: SQL execution error: Creating stage on shared database 'SNOWFLAKE_SAMPLE_DATA' is not allowed.\n\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:170)\n\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowException(SnowflakeUtil.java:103)\n\tat net.snowflake.client.core.StmtUtil.pollForOutput(StmtUtil.java:501)\n\tat net.snowflake.client.core.StmtUtil.execute(StmtUtil.java:407)\n\tat net.snowflake.client.core.SFStatement.executeHelper(SFStatement.java:498)\n\tat net.snowflake.client.core.SFStatement.executeQueryInternal(SFStatement.java:215)\n\tat net.snowflake.client.core.SFStatement.executeQuery(SFStatement.java:149)\n\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:785)\n\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:693)\n\tat net.snowflake.client.jdbc.SnowflakeStatementV1.executeQueryInternal(SnowflakeStatementV1.java:296)\n\tat net.snowflake.client.jdbc.SnowflakePreparedStatementV1.executeQuery(SnowflakePreparedStatementV1.java:157)\n\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executePreparedQueryInterruptibly$1(SnowflakeJDBCWrapper.scala:229)\n\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executeInterruptibly$2(SnowflakeJDBCWrapper.scala:274)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:356)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:568)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1171)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:493)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:455)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:310)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:170)\n\t\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowException(SnowflakeUtil.java:103)\n\t\tat net.snowflake.client.core.StmtUtil.pollForOutput(StmtUtil.java:501)\n\t\tat net.snowflake.client.core.StmtUtil.execute(StmtUtil.java:407)\n\t\tat net.snowflake.client.core.SFStatement.executeHelper(SFStatement.java:498)\n\t\tat net.snowflake.client.core.SFStatement.executeQueryInternal(SFStatement.java:215)\n\t\tat net.snowflake.client.core.SFStatement.executeQuery(SFStatement.java:149)\n\t\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:785)\n\t\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:693)\n\t\tat net.snowflake.client.jdbc.SnowflakeStatementV1.executeQueryInternal(SnowflakeStatementV1.java:296)\n\t\tat net.snowflake.client.jdbc.SnowflakePreparedStatementV1.executeQuery(SnowflakePreparedStatementV1.java:157)\n\t\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executePreparedQueryInterruptibly$1(SnowflakeJDBCWrapper.scala:229)\n\t\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executeInterruptibly$2(SnowflakeJDBCWrapper.scala:274)\n\t\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\t\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\t\tat scala.util.Success.map(Try.scala:213)\n\t\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\t\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\t\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\t\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t\t... 1 more\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/root/.ipykernel/1099/command-6923127474726906-1897863283\", line 44, in advanced_snowflake_operations\n    .save()\n     ^^^^^^\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 47, in wrapper\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 1732, in save\n    self._jwrite.save()\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py\", line 1362, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 269, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py\", line 327, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o624.save.\n: net.snowflake.client.jdbc.SnowflakeSQLException: SQL execution error: Creating stage on shared database 'SNOWFLAKE_SAMPLE_DATA' is not allowed.\n\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:170)\n\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowException(SnowflakeUtil.java:103)\n\tat net.snowflake.client.core.StmtUtil.pollForOutput(StmtUtil.java:501)\n\tat net.snowflake.client.core.StmtUtil.execute(StmtUtil.java:407)\n\tat net.snowflake.client.core.SFStatement.executeHelper(SFStatement.java:498)\n\tat net.snowflake.client.core.SFStatement.executeQueryInternal(SFStatement.java:215)\n\tat net.snowflake.client.core.SFStatement.executeQuery(SFStatement.java:149)\n\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:785)\n\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:693)\n\tat net.snowflake.client.jdbc.SnowflakeStatementV1.executeQueryInternal(SnowflakeStatementV1.java:296)\n\tat net.snowflake.client.jdbc.SnowflakePreparedStatementV1.executeQuery(SnowflakePreparedStatementV1.java:157)\n\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executePreparedQueryInterruptibly$1(SnowflakeJDBCWrapper.scala:229)\n\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executeInterruptibly$2(SnowflakeJDBCWrapper.scala:274)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:356)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:568)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1171)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:493)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:455)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:310)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:170)\n\t\tat net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowException(SnowflakeUtil.java:103)\n\t\tat net.snowflake.client.core.StmtUtil.pollForOutput(StmtUtil.java:501)\n\t\tat net.snowflake.client.core.StmtUtil.execute(StmtUtil.java:407)\n\t\tat net.snowflake.client.core.SFStatement.executeHelper(SFStatement.java:498)\n\t\tat net.snowflake.client.core.SFStatement.executeQueryInternal(SFStatement.java:215)\n\t\tat net.snowflake.client.core.SFStatement.executeQuery(SFStatement.java:149)\n\t\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:785)\n\t\tat net.snowflake.client.core.SFStatement.execute(SFStatement.java:693)\n\t\tat net.snowflake.client.jdbc.SnowflakeStatementV1.executeQueryInternal(SnowflakeStatementV1.java:296)\n\t\tat net.snowflake.client.jdbc.SnowflakePreparedStatementV1.executeQuery(SnowflakePreparedStatementV1.java:157)\n\t\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executePreparedQueryInterruptibly$1(SnowflakeJDBCWrapper.scala:229)\n\t\tat net.snowflake.spark.snowflake.JDBCWrapper.$anonfun$executeInterruptibly$2(SnowflakeJDBCWrapper.scala:274)\n\t\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\t\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\t\tat scala.util.Success.map(Try.scala:213)\n\t\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\t\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\t\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\t\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t\t... 1 more\n\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Advanced Operations with Comprehensive Error Handling\n",
    "def advanced_snowflake_operations(sf_options):\n",
    "    try:\n",
    "        print(\" Starting advanced operations...\")\n",
    "        \n",
    "        # Configuration for large datasets\n",
    "        optimized_options = sf_options.copy()\n",
    "        optimized_options.update({\n",
    "            \"parallelism\": \"8\",\n",
    "            \"usestagingtable\": \"on\",\n",
    "            \"autopushdown\": \"on\"\n",
    "        })\n",
    "        \n",
    "        # Read with optimized settings\n",
    "        large_df = spark.read \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**optimized_options) \\\n",
    "            .option(\"dbtable\", \"LINEITEM\") \\\n",
    "            .load()\n",
    "            # .limit(50000)  # Optional limit for testing\n",
    "        \n",
    "        print(f\" Loaded LINEITEM table with {large_df.count()} rows\")\n",
    "        \n",
    "        # Perform some transformations\n",
    "        transformed_df = large_df \\\n",
    "            .filter(col(\"L_SHIPDATE\") >= \"1998-01-01\") \\\n",
    "            .groupBy(\"L_SHIPDATE\") \\\n",
    "            .agg(\n",
    "                sum(\"L_QUANTITY\").alias(\"total_quantity\"),\n",
    "                avg(\"L_EXTENDEDPRICE\").alias(\"avg_price\"),\n",
    "                count(\"*\").alias(\"transaction_count\")\n",
    "            ) \\\n",
    "            .orderBy(\"L_SHIPDATE\")\n",
    "        \n",
    "        print(\"Transformed data (aggregated by ship date):\")\n",
    "        display(transformed_df.limit(10))\n",
    "        \n",
    "        # Write transformed data back to Snowflake\n",
    "        transformed_df.write \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**optimized_options) \\\n",
    "            .option(\"dbtable\", \"LINEITEM_AGGREGATED\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "        \n",
    "        print(\" Successfully wrote aggregated data to 'LINEITEM_AGGREGATED'\")\n",
    "        \n",
    "        return transformed_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error in advanced operations: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "advanced_result = advanced_snowflake_operations(sfOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c5419d2-8c9f-4112-a2a2-3044480d02d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Connecting Databricks to Snowflake ip",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}